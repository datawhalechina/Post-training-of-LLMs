在本节课中，您将学习后训练方法的基本概念。让我们开始深入探讨。

首先了解什么是后训练。通常训练语言模型时，我们会从随机初始化的模型开始进行预训练。这个阶段的目标是从各类数据源学习知识，包括维基百科、全网爬取的`Common Crawl`数据，或`GitHub`的代码数据。完成预训练后，我们将获得一个基础模型，它能够预测下一个词或标记——如图所示，每个标记代表一个子词单元。

以此基础模型为起点，下一步我们将进行后训练，其核心目标是从精心筛选的数据中学习响应模式。这类数据包括对话数据、工具使用数据或智能体数据。通过此过程，模型将升级为指令模型或对话模型，能够对指令作出响应或与用户进行交流。当被问及"巴黎是哪个国家的首都"时，模型将能准确回答"巴黎是法国的首都"。

在此基础之上，我们还可以进一步开展后训练，以调整模型行为或增强特定能力。最终我们将获得一个定制化模型，该模型可专精于特定领域或具备特定行为模式。例如在本案例中，模型将能够针对各类指令生成更优质的SQL查询语句。

为了更深入理解后训练方法，我们先从预训练方法入手。预训练通常被视为无监督学习，其起点是大规模无标注文本语料（如维基百科、`Common Crawl`或`GitHub`等）。通常可从这些语料中提取超过2万亿个标记进行训练。以最小示例说明，当输入"我喜欢猫"这样的句子时，模型会基于前面所有标记来最小化每个标记的负对数概率：首先最小化"我"的负对数概率，然后是给定"我"时"喜欢"的负对数似然，最后是给定"我喜欢"时"猫"的概率。通过这种方式，模型被训练成能根据已见标记预测下一个标记。

完成预训练后，接下来会采用不同的后训练方法：

1. **监督微调**：作为最简单且最流行的后训练方法，它属于监督学习/模仿学习范畴。需要创建包含标注的提示-响应对数据集，其中提示通常是给模型的指令，响应则是模型应有的理想回答。此过程仅需1,000至10亿个标记，远少于预训练规模。其训练损失的关键区别在于：仅对响应标记进行训练，而不涉及提示标记。

2. **直接偏好优化**：该方法需要创建包含提示及其对应优质/劣质响应的数据集。针对任一提示，可生成多个响应并筛选出优质与劣质样本。训练目标是使模型远离劣质响应并学习优质响应。该方法同样仅需1,000至10亿个标记，并采用更复杂的损失函数（后续课程将详细展开）。

3. **在线强化学习**：此方法只需准备提示集和奖励函数。从提示开始，让语言模型生成响应，再通过奖励函数对该响应进行评分，最后利用该信号更新模型。通常需要1,000至1,000万（或更多）个提示，目标是通过模型自身生成的响应来最大化奖励值。


成功的后训练需要确保三个关键要素：

1. **数据与算法的协同设计**：如前所述，后训练有多种方法选择（`SFT`、`DPO`及各在线强化学习算法等），每种方法所需的数据结构略有不同。良好的协同设计对后训练成效至关重要。

2. **可靠高效的算法库**：`HuggingFace TRL`作为首批易用库之一，实现了大部分前述算法，本课程将主要使用该库进行编程实践。此外还推荐`Open RLHF`、`veRL`和`Nemo RL`等更精密、内存效率更高的库。

3. **合适的评估体系**：需通过完善的评估方案追踪模型在后训练前后的表现，确保模型性能持续优良。现有流行语言模型评估标准包括：
    
    - 对话机器人竞技场：基于人类偏好的聊天评估
        
    - 替代人类评判的LLM评估：`AlpacaEval`、`MT Bench`、`Arena Hard`
        
    - 指令模型静态基准：`LiveCodeBench`（热门代码基准）、`AIME 2024/2025`（高难度数学评估）
        
    - 知识与推理数据集：`GPQA`、`MMLU Pro`
        
    - 指令遵循评估：`IFEval`
        
    - 函数调用与智能体评估：`BFCL`、`NexusBench`、`TauBench`、`ToolSandbox`（后两者专注多工具使用场景）


需要强调的是，提升单一基准成绩相对容易，但要在不损害其他领域能力的前提下改进特定行为则更具挑战。本课程将重点探索如何实现此目标。

最后，并非所有用例都需要进行模型后训练：

- 若仅需模型遵循少量指令（如回避敏感话题或禁止公司间比较），通过提示工程即可实现，但该方法虽简单却不够稳定

- 如需查询实时数据库，检索增强生成或基于搜索的方法可能更适用

- 创建领域专用模型（如医疗或网络安全语言模型）时，通常需要持续预训练结合标准后训练，先让模型学习领域知识（至少需10亿标记），再学习用户交互

- 当需要严格遵循20条以上指令，或提升特定能力（如构建强SQL模型、函数调用模型或推理模型）时，后训练最能发挥价值——它能可靠改变模型行为并提升目标能力，但若实施不当可能导致其他未训练能力退化


通过本课程学习，您已掌握后训练的定义、方法与实践场景。下节我们将深入探讨首个后训练方法：监督微调。期待与您继续探索！