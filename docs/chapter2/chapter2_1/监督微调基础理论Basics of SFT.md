# 语言模型的监督式微调（SFT）

## 概述

监督式微调（Supervised Fine‑Tuning, SFT）是一种把通用语言模型转换成任务型助手的方法。通过训练提示与理想回应的成对数据，模型学会模仿示例中的回答，从而能够按照指令行事、展示期望的行为并正确调用工具。


SFT 的核心是让基础模型（只根据提示预测下一个 token）学会生成预期的回答，流程如下：

- **基础模型**：未经调整的 LLM 往往会给出泛泛或重复的回应，例如面对“你是谁？”这样的询问，它可能只是反问一句，而不是回答。
- **带标签的数据集**：收集并整理用户提示与理想助理回应的配对，例如“请告诉我你的身份——我是 Llama…”、“你最近怎么样？——我很好！”。
- **SFT 训练**：对这些配对进行微调，通过最小化回应的交叉熵损失来训练模型：

$$\mathcal{L}_{\text{SFT}} = -\sum_{i=1}^N \log \bigl(p_\theta(\text{Response}(i)\mid \text{Prompt}(i))\bigr)$$


这一损失鼓励模型最大化在每个提示条件下生成目标回应的概率。


- **微调后的模型**：完成训练后，模型可以针对新的查询给出合适的回复（例如向用户问好，而不是简单重复问题）。

![SFT 流程示意图](../../images/SFT1.png)

上面的公式也可以理解为最大化在提示条件下回应中所有 token 的联合概率。交叉熵损失会惩罚偏离标签回应的输出，因此 SFT 本质上是在教模型“模仿”。

## SFT 的最佳使用场景

SFT 并非万能，它在特定场景下尤其有效：

- **激发新的模型行为**：
  - 将预训练模型转变为能遵循指令的助理。
  - 让不具备推理能力的模型学会基本推理。
  - 让模型在没有明确说明的情况下使用特定工具。
- **提升模型能力**：
  - 利用强大的大模型生成高质量的合成数据，通过训练把这些能力“蒸馏”到小模型中。

![SFT 的最佳应用场景](../../images/SFT2.png)

这些例子显示了 SFT 作为预训练和更高级对齐方法之间的桥梁作用。当你需要模型快速适应新行为且有示例数据时，SFT 往往是正确的选择。

## SFT 数据策划原则

SFT 的效果高度依赖于数据质量。优质且多样的样本能让模型学到有用的行为；劣质样本则会让模型模仿不良习惯。常用的数据策划方法包括：

- **蒸馏**：用更强的指令模型生成回复，再训练小模型去模仿这些回复，把强模型的能力迁移到弱模型上。
- **Best‑of‑K / 拒绝采样**：针对同一提示生成多个候选回复，再用奖励函数选出最好的作为训练数据。
- **过滤**：从大型 SFT 数据集中挑选出回应质量高且提示多样性好的样本，形成精简的高质量数据集。

这里的核心是“质量比数量重要”。一千条精心挑选、题材丰富的样本往往比一百万条参差不齐的数据效果更好，因为 SFT 会迫使模型模仿它所见到的一切——包括糟糕的回答。

![SFT 数据策划原则](../../images/SFT3.png)

## 全参数微调 vs 参数高效微调

在执行 SFT（或其他对齐方法）时，需要决定如何更新模型权重：

![全参数微调与参数高效微调对比](../../images/SFT4.png)

- **全参数微调**：对每一层加入一个完整的权重更新矩阵  $\Delta W$ ，即修改所有参数。这可以显著提升性能，但需要大量存储和计算资源。
- **参数高效微调**：例如 LoRA（低秩适配）通过在每层引入小的低秩矩阵 A 和 B 来调整模型参数。这减少了可训练参数的数量，节省显存，缺点是学习和遗忘都更有限，因为更新的参数更少。

这两种策略可以与任何训练方法结合。根据资源约束和性能要求，你可以选择全微调或参数高效微调。后者在硬件条件有限的情况下尤为受欢迎。

## 总结

监督式微调是语言模型对齐的重要基础方法。它通过最小化目标回复的负对数似然，使模型学会模仿期望的行为并在面对提示时做出合适回应。SFT 特别适合用于启动新行为和从大模型向小模型“蒸馏”能力。然而，数据质量至关重要：蒸馏、拒绝采样和过滤等策略能显著胜过简单堆积大量普通数据。全参数与参数高效微调之间的选择则是性能和资源之间的权衡。

